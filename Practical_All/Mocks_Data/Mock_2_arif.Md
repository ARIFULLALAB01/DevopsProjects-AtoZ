** Mock Interview -2 **

0:01 - Intro and Introduction to Candidate's Profile
1:12 - Candidate's Experience and Current Role at Golden City Digital Technologies
3:12 - Multi-Cloud Experience and Strong Cloud Platform (AWS)
4:09 - Focus on Cost Optimization and Services Offered
5:40 - Detailed Discussion on Cost Optimization Strategies and Tools
9:00 - Transition to DevOps and Training Experience
10:22 - Overview of DevOps Concepts and Tools (e.g., Jenkins, Terraform)
13:02 - Job Market and Current Job Search Experience
14:15 - Linux Skill Set and Troubleshooting Scenario
15:46 - Understanding of Public and Private Subnets
17:00 - Explanation of AWS S3 Bucket Policy
19:05 - Terraform Code Discussion and Explanation
21:52 - Dockerfile Explanation and Discussion
25:05 - Overview of Jenkins and Jenkinsfile
27:06 - CloudWatch Monitoring and Agent Usage
30:25 - Discussion on VPC Flow Logs and Log Streaming
31:29 - Closing Remarks and Candidate's Interest in DevOps Technology

*  Intro and Introduction to Candidate's Profile: 
1) if you can help me explain a little bit about your profile what sort of skill set you have what sort of experience you bring some of the responsibilities that you have been handling in your last role and from there we can started:
sure definitely yes :I do have four years of experience my current company is Golden City digital Technologies where I'm working as a cloud engineer so here I got an opportunity to explore three different clouds that's AWS AZURE and GCP  where the major Focus was on Philips cost optimization methodologies like we worked on different services in AWS like including EKS, redshift, EMR, ECR cloud watch CloudTrail so what we usually do is we analyse the service we test the service using APIS and clis we also use Postman and we are also using MongoDB and SQL DB we use Robo 3T for that time does for SQL we do use SQL studio so we analysed service test the service and then Define the policy so we do have subcategory in Phenops like idle service being idle it is being orphaned and I am majorly focused on cloud Technologies what do you offer through that do you train people build some products yes we are also focusing on building a product as well but not yet doublyfledgedum like we are integrating different Technologies uh like we do have devops tools so we are trying out new methods but as of now we provide services to clients that's still a major Focus as of now
so it's a service provider like every analysis okay got it so which isfor criteria networks for now currently working for code stack and I'm also planning to work for a terraform as well that’s with synedria a U.S client yeah that's the new project we are getting okay so you work with different uh these customers and then you implement based on their skills their requirements  what they require yes we provide the service and we are also exploring new products as well to build as our company owned product cool and so it's a multi-cloud experience
2) * Multi-Cloud Experience and Strong Cloud Platform (AWS)
 you said right all the cloud providers and okay all three clouds okay so which is your strong uh area then which is your strong Cloud platform that you work on uh AWS okay and but as of now I'm also comfortable with I sure because I worked for almost one and a half year and as well as the GCP but most of the services are from AWS and Azure gcp a few of them all together around 40plus Services we have worked on and we do provide the entire logic like using we provide it's you have those that experience right not your company yes not my company it's uh you are working on that right okay so uh I mean I was just telling the team I worked on like 20 plus services so we do have a team where we coordinate with each other we review whatever the policies they have defined or I have defined so that way we have exposure to many services okay I see you are searching it’s like 50 plus so much yes Associate and these are valid certifications right now yes that’re tall right okay I I got a good idea uh on the profile
3) * Focus on Cost Optimization and Services Offered :
* so I I wanted to understand you mentioned a bit about cost optimization so what sort of is it a customer requirement to work on cost optimization yeah so uh post stack mainly focus on phenops so they want to reduce the cost in the client accounts so what we do is first we test in the dev environment how our logic works so for example if I have to give a simple say ec2 instance uh now we go with uh say idle policy so what do we do is we check with the metrics for example CPU and memory utilization we have a threshold say five percent if itis less than that then the customer is not using and it is a waste of time having that in their environment so we tell that customers that uh this is of no use since you're not using it for a certain period and there'll be also provide them how the cost works as well whether it is billing role logic or unit rate logic we have complex policies as well so which have really helped the customers in cutting down the cost okay.
4) * Detailed Discussion on Cost Optimization Strategies and Tools:
 but then this there are a lot of tools right they provide these kinds of reports there is is there anything beyond that uh do you like identifying idle resources one case for uh optimization anything else udo you understand reserved instances
 yes spot institutions Reserve dedicated host is we not only worked on the simple terms a complex conflict policies Aswell for example a service might have different compute tires and for example if I talk about file store in gcp it has Enterprise basic standard so we analyse what Enterprise features are what standard features are and what basic features are whether customer is using all those features to its Optimum level if it's not being used to how to detect that and how cost reducing happens if we ask them to switch over that and we also recommend them as well so based on then we just provide what all the alternative recommendations they can use but it’s completely based on client whichever the recommendations suitable for them they’re going to use that okay for example if some customer is running their workloads in EKS maybe what all areas you will analyse for cost optimizations for example uh AWS e case first we’ll check whether we have a master uh node and worker nodes so we'll check whether the essence master node is a password we will not be doing anything there worker node will analyse what type of instance they are using it whether they they are using it to its Optimum level if that particular instances instance type is not being used to its optimal levels fora GCP whatever I mean memory and CPU whatever the instance type provide if they are not using will ask them to shift to a distance a different instant type and also we do have pods as well so we do recommend the horizontal scaling vertical or Auto scaling if the pods are not being used we do have metrics for pod using Cube CTL we analyse those metrics whether the pods are being used completely or not on board level or instance level on container level also weighted but not so and if you have to optimize the container images that also is a big space type like big challenge yes yes it’s a complex method okay all right okay .

*) Detailed Discussion on Cost Optimization Strategies and Tools :
*) what about your devops  area are you comfortable yes from past five months uh I’ve been trained on that uh basically all Kubernetes so all the basic concepts like what is devops uh what are the tools used in devops how it works so basically I understand sort of you would use sure so in devops we first go with the planning and then the source code and then build testing and then we have after testing we do release and then operate and monitor continuous integration continuous deployment and then continuous  development continuous first is continuous development continuous integration continuous deployment and then the configuration management and then we have a continuous monitoring so first if we I'm using devops planning Tool uh that's nothing but assured boats we are using from past two years where we plan uh what is the project about and what comes under the projects like epic under that we do have subcategories as features and then we have user story and then tasks so that's the planning tool which I'm using uh that's actually reports and next comes the source code source code a bitbucket uh SVN and GitHub mainly I've worked on now or GitHub only so um uh GitHub I have experience on git stash uh stash conflict how to solve that and master Branch how to create a user branch and merge it to the a remote repo from local to uh remote and then pull requests so that’s I've worked on I've not worked on SVN and big bucket so only GitHub and then so next comes to containerization Tool integration tool containerization tools Docker compose Docker file and then we have integration tool Jenkins the bamboo worked on Jenkins so where I'll create build pipeline I worked on amazing how will Jenkins talk to Azure or Azure devops you you mentioned started with uh yes I have implemented using a Helm charts during uh so we create what I have done is I have created a Usher AKS uh in Azure portal and then um I have then implemented Jenkins on it using the helm chart and then I have manually Aswell as scripted not much but manually I’ve done a lot of things like building the pipeline uh terraform yes time I’m learning a little bit of this one terraform is infrastructure code just to automate the infrastructure project where you have a case I have not worked on uh AWS uh eks but Azure a case I have tried with the terraform code still I am learning so we do have a new project coming up uh where we have to give a test and then enter into the project so for that we are still working on ityes we have used the telephone codes using terraform you have created the AKS in Azure portal you yes you are giving conflicting me messages you have created in Azure portal or you have created the cluster using terraform or both I have tried out first I have need to know how it works in the portals after which I have shifted to a code because that's the automation because  I usually what I do is I go with the portal experience like how exactly it works and then with the codes that's easy to see how it works and we also provided cost optimization on that uh and then um telephone code that we have used okay do you have any experience with ansible no much experience configuration management tool for upgrades uh we're going to use a Playbook Centre yes I don’t have much experience but I love to work on those tools as well because at other companies
*) Job Market and Current Job Search Experience
 As well like how many weeks have you started trying for new position and it just started from last week okay how is the market are you getting a lot of calls how is the scenario from past one week I just got through two three calls one is for uh cognizant the other is for reminding okay cool the market is good you're getting costs yes and Cloud right yes based on your mainly you have Cloud experience I believe right 
*) Linux Skill Set and Troubleshooting Scenario:
how is your Linux skill set initially 
I worked on Linux itself so I do have good experience on that where I used to manage is manage uh user accounts creating the users and then providing permissions ACL Access Control list and then you have to look at some log file which is being populated in parallel How would you repeat that file Slash word slash locks where you will get the information about whatever the user have created or done and their file is being populated right now you want to monitor that log file in real times you can't you can't look at the last hundred lines because the last hundred lines will be overwritten very shortly and then there will be fresh lines added to it to you want me to monitor a log file file you want to monitor log file to give get some errors or something like that okay get some errors but the data is usually in the late last written blogs right last written 10 or15 or 100 lines but the log file is getting populated like it's being overwritten every or not overwritten but frequently updated we can use GreP command in the logfile whichever we want to look into itso your voice is breaking yeah is it better not yes okay uh on the 

*) Understanding of Public and Private Subnets
 AWS or Azure what is this public subnet and private subnets do you understand what is the main difference between all these differences yes so public sermons subnets where uh user is giving we give all the traffic to that public subnet where user can access but private subnets only the private IPS are allowed not the public IPS by other internet so for public subnet we basically don’t use net Gateway for private subnet we douse lab Gateway or some other like connecting with other VPC but not with the public directly on contact okay I'm just trying to see based on your resume and where you would fit into is  there a any sort of scripting experience you have can you write some code as well I’ve written for uh that like terraform like I told I have done it for AZURE case and still learning that and probably two weeks or three weeks I’ll catch it with terraform so no problem can you uh 
*) Explanation of AWS S3 Bucket Policy
give me or I will just maybe will find some policy and maybe you can help me explain that bucket policy okay so let me send something in the chat box and see if you can explain this policy okay so we are allowing public access to the yea it's on the screened so we are allowing public access to the resource in order to get objects from that specific year in that specific bucket what does the star means okay example bucket slash I think any object from that bucket okay if the then object is resource is this one right yesso inside that bucket we do have objects what is this the one on screen “principle : * ”I'm not getting it  think any object can be picked from the buckeye correct that is correct I just wanted to understand what does this star means you got it right I think maybe you are getting some idea from this line or butI wanted to understand on what is this principle star means okay principal star and what does this action means action is where you perform any tasks so your uh your action is to get the object from Mystery buckets this is the resource where which bucket we have this is the action then what what would this be this possibly is the user right where who okay right who is trying to access star  means uh public uh you can say it’s not specified any user or role there okay can you 
*) Terraform Code Discussion and Explanation  (or) Dockerfile Explanation and Discussion :
write a bit of terraform code or something you can explain Aswell uh or or if any any of the language where you are comfortable with you can write a some simple use case maybe a hello world in that project okay maybe a Docker file and just  keep explaining like what what you are writing and while you are writing okays well I just unknow how it works when I see the code but right now I'm not able to remember it just just a simple even if it is not corrective syntax wise or something I want to understand whether you know conceptually and I believe you are certified so you would know those sort of things that that's the level I I need to understand or if you want I can I can pull a Docker file and you can explain Michael please take me some time okay let I have a file let me uh share it and that’s okey first we're going to pull the base image that is the container base image that will be a node and the latest version as for clean and then directory inside the container what does this specify the version and next we are creating a contained a working directory in a container because Docker file will be of same name but folder should be different so we're Gonna create a folder slash uh ABPand then copy the package Json and package log Json file to the working directory okay so we are copying this package.jsonto this uh working directory that's happened then you're going to install all the dependencies using current command so we’re always used basically when you want to instal link packages from VM repo EBT so that will install the packages so while building the image and next copy the rest of the application code to the working directory yes dot uh the application code with extension if it is a Linux or a Windows dot exe or if it's a Linux we have dot word or jar so whatever the extension file we're going to copy it and then we’re going to export the expose the port number so here the port number is three thousand so we're going to export uh expose that port number what does this mean to expose uh the port number uh in the sense we're going to expose that application on the specific port number that’s uh 3000 and then yes CMD is used to uhoh actually to open not a paint to replace the packages or why local dinner is running okay packages or any statements to the Target location if you want to replace while container is running we'll use that command form to start the application yes ok all right some of the comments would have helped sorry some of the comments in the code would have helped you a little bit oryouoh no I know the structure of the Docker file so it was simple good all right 
*) Overview of Jenkins and Jenkinsfile
So what area other than that we can I wanted to understand a bit of Jenkin file do you understand what the Jenkins file issue yes so Jenkins files were um uh I don’t have much experience on that Jenkins have done all the manual activities so for Jenkins we do have a declarative and descriptivism where we going to specify them build a pipeline task like if I take me when we do have six tasks so the task it can be a scripted in Jenkins file and then the jobs will be sorry I'm not able to see your videoed I'm there yes yeah so automatically all the tasks which has been uh which has been specified by the ability tool it will be done like for me when we have some six tasks code review compile unit test uh metric check package deploy so all that can be unjust uh dumped into into a code and that will perform the job okay 
*) CloudWatch Monitoring and Agent Usage:
 can you tell some experience you earlier mentioned about uh CloudWatch agent or Cloud watch monitoring what sort of experience you have cloud watch basically we monitor the Matrix of various resources we have also set up the alarm like whenever the threshold increases the alarm has to populate and a user has to get the notification I worked on SNS monitoring in Cloud watch and there is also a cloud watch agent when do we need agent cloud watch agent I've used for use AWS -EKS we do require cloud watch agent okay why do we need agent what's the advantage and disadvantage like because when I create an ec2 instance I do get some metrics out of the box I don’t need an agent for that you’re right what specific use cases do we need agent for the cloud watch agent specific use cases do remember that it is for a few services we do require your Cloud watch agent but not able to remove this remember the specific use case any use case because I don't need agent for CPU and basic stuff like just to get the CPU and RAM kind of thing why do I need agent at all then right for specific metrics which will not be available in CloudWatch that we can populate in CloudWatch agent for example like any example of that sort of metrics uh pod metrics like we do have  the number of pods you know have you configured any logs to be sent over CloudWatch assuming the log stream yeah yes I had configured lock screen so you would need an agent configured for that rights you mean foreign for that okay no logs as in PPC flow logs backflow logs
*) Discussion on VPC Flow Logs and Log Streaming :
 what are VPC flow logs where we get information about the network like the IP’s which have pink that vc2 instance you I think you are mixing up to Concepts right like or is it me not asking I want some logs from the C2instance not the VPC flow data blocks from easy to install something some you you earlier mentioned like have a slash War slash log has some data I want that in Cloud watch logs group so anyway okay so you need agent for such cases you need agent installed on that ec2 instance and then that can stream the logs to logs group you bring and uh where you can add a lot of value anything you you want to ask before we close the discussions of no yeah I'm good with litany 

*) Closing Remarks and Candidate's Interest in DevOps Technology
any last comment you want to make before we close yes I am too interested in Devops technology and that's the reason I’m opting for uh different options so here I’ve got a recent I've got a terraform project where I'll be working on it but still want to work on devops tools so I’m very much interested in that soyes I'll be happy to drop them lots of such projects and opportunities so yeah thank you all right then have then we'll close the discussion and while share the feedback sure thank you thank you thanks take care bye thank you bye-
